"""
Tests unitaires pour le mod√®le KNN de classification par secteur
Fichier: tests/test_knn_classification.py
"""

import unittest
import pickle
import pandas as pd
import numpy as np
from pathlib import Path
import sys

# Ajouter le dossier src au path
sys.path.append(str(Path(__file__).parent.parent / 'src'))


class TestKNNClassification(unittest.TestCase):
    """Tests pour le mod√®le KNN de classification"""
    
    @classmethod
    def setUpClass(cls):
        """Charge les mod√®les une seule fois pour tous les tests"""
        print("\n" + "="*80)
        print("üß™ TESTS UNITAIRES - KNN CLASSIFICATION")
        print("="*80)
        
        models_dir = Path(__file__).parent.parent / 'models'
        
        # Charger le mod√®le KNN
        model_path = models_dir / 'knn_classification_secteur.pkl'
        with open(model_path, 'rb') as f:
            cls.model = pickle.load(f)
        
        # Charger le scaler
        scaler_path = models_dir / 'scaler_classification_secteur.pkl'
        with open(scaler_path, 'rb') as f:
            cls.scaler = pickle.load(f)
        
        # Charger la config
        config_path = models_dir / 'knn_classification_secteur_config.pkl'
        with open(config_path, 'rb') as f:
            cls.config = pickle.load(f)
        
        print("‚úÖ Mod√®les charg√©s")
    
    def test_01_model_exists(self):
        """Test 1: V√©rifier que le mod√®le existe"""
        print("\nüîç Test 1: V√©rification de l'existence du mod√®le")
        self.assertIsNotNone(self.model)
        print("   ‚úÖ Mod√®le KNN existe")
    
    def test_02_model_type(self):
        """Test 2: V√©rifier le type du mod√®le"""
        print("\nüîç Test 2: V√©rification du type de mod√®le")
        from sklearn.neighbors import KNeighborsClassifier
        self.assertIsInstance(self.model, KNeighborsClassifier)
        print(f"   ‚úÖ Type correct: {type(self.model).__name__}")
    
    def test_03_model_parameters(self):
        """Test 3: V√©rifier les param√®tres du mod√®le"""
        print("\nüîç Test 3: V√©rification des param√®tres")
        # V√©rifier que n_neighbors correspond √† la config
        expected_k = self.config.get('n_neighbors', 3)
        self.assertEqual(self.model.n_neighbors, expected_k, 
                        f"n_neighbors devrait √™tre {expected_k}")
        print(f"   ‚úÖ n_neighbors: {self.model.n_neighbors}")
        print(f"   ‚úÖ weights: {self.model.weights}")
    
    def test_04_scaler_exists(self):
        """Test 4: V√©rifier que le scaler existe"""
        print("\nüîç Test 4: V√©rification du scaler")
        self.assertIsNotNone(self.scaler)
        from sklearn.preprocessing import StandardScaler
        self.assertIsInstance(self.scaler, StandardScaler)
        print("   ‚úÖ Scaler existe et est du bon type")
    
    def test_05_config_structure(self):
        """Test 5: V√©rifier la structure de la configuration"""
        print("\nüîç Test 5: V√©rification de la configuration")
        # Cl√©s obligatoires dans ta config
        required_keys = ['classes', 'n_neighbors', 'features']
        for key in required_keys:
            self.assertIn(key, self.config, f"Cl√© '{key}' manquante dans config")
        
        print(f"   ‚úÖ Configuration compl√®te avec {len(self.config)} cl√©s")
        print(f"   Classes disponibles: {self.config['classes']}")
        print(f"   Nombre de features: {len(self.config['features'])}")
        
        # V√©rifier les performances si disponibles
        if 'performance' in self.config:
            print(f"   Accuracy: {self.config['performance']['accuracy']:.2%}")
    
    def test_06_prediction_shape(self):
        """Test 6: V√©rifier la forme des pr√©dictions"""
        print("\nüîç Test 6: Test de pr√©diction - forme du r√©sultat")
        
        try:
            # Cr√©er des donn√©es de test fictives
            n_features = self.model.n_features_in_
            X_test = np.random.rand(5, n_features)
            X_test_scaled = self.scaler.transform(X_test)
            
            predictions = self.model.predict(X_test_scaled)
            
            self.assertEqual(len(predictions), 5, "Devrait pr√©dire 5 secteurs")
            print(f"   ‚úÖ Pr√©dictions: {predictions}")
        except Exception as e:
            self.fail(f"Erreur lors de la pr√©diction: {e}")
    
    def test_07_prediction_classes(self):
        """Test 7: V√©rifier que les pr√©dictions sont des classes valides"""
        print("\nüîç Test 7: Validation des classes pr√©dites")
        
        try:
            n_features = self.model.n_features_in_
            X_test = np.random.rand(10, n_features)
            X_test_scaled = self.scaler.transform(X_test)
            
            predictions = self.model.predict(X_test_scaled)
            valid_classes = self.config['classes']
            
            for pred in predictions:
                self.assertIn(pred, valid_classes, f"Classe '{pred}' invalide")
            
            print(f"   ‚úÖ Toutes les pr√©dictions sont des classes valides")
        except Exception as e:
            self.fail(f"Erreur lors de la validation: {e}")
    
    def test_08_prediction_probabilities(self):
        """Test 8: V√©rifier les probabilit√©s de pr√©diction"""
        print("\nüîç Test 8: Test des probabilit√©s")
        
        try:
            n_features = self.model.n_features_in_
            X_test = np.random.rand(3, n_features)
            X_test_scaled = self.scaler.transform(X_test)
            
            probas = self.model.predict_proba(X_test_scaled)
            
            # V√©rifier que les probabilit√©s somment √† 1
            for i, proba_row in enumerate(probas):
                sum_proba = np.sum(proba_row)
                self.assertAlmostEqual(sum_proba, 1.0, places=5, 
                                     msg=f"Somme des probas pour CV {i} != 1")
            
            print(f"   ‚úÖ Probabilit√©s valides (somme = 1.0)")
            print(f"   Exemple de probabilit√©s: {probas[0]}")
        except Exception as e:
            self.fail(f"Erreur lors du calcul des probabilit√©s: {e}")
    
    def test_09_feature_count(self):
        """Test 9: V√©rifier le nombre de features"""
        print("\nüîç Test 9: V√©rification du nombre de features")
        
        n_features_config = len(self.config['features'])
        n_features_model = self.model.n_features_in_
        
        self.assertEqual(n_features_config, n_features_model, 
                        "Nombre de features incoh√©rent")
        print(f"   ‚úÖ Nombre de features: {n_features_model}")
        print(f"   Features: {self.config['features']}")
    
    def test_10_realistic_cv_prediction(self):
        """Test 10: Test avec un CV r√©aliste"""
        print("\nüîç Test 10: Test avec donn√©es r√©alistes")
        
        try:
            # Utiliser directement le bon nombre de features du mod√®le
            n_features = self.model.n_features_in_
            
            # Simuler un CV avec des valeurs r√©alistes
            # On cr√©e un vecteur avec des valeurs moyennes pour toutes les features
            X_test = np.array([[450, 12, 1, 1, 1, 2, 8, 0.027, 2.67] + [0] * (n_features - 9)])
            
            # S'assurer qu'on a le bon nombre de features
            X_test = X_test[:, :n_features]
            
            X_test_scaled = self.scaler.transform(X_test)
            prediction = self.model.predict(X_test_scaled)
            probas = self.model.predict_proba(X_test_scaled)
            
            print(f"   ‚úÖ Secteur pr√©dit: {prediction[0]}")
            print(f"   Confiance: {np.max(probas):.2%}")
            
            # Afficher top 3 secteurs
            top_indices = np.argsort(probas[0])[::-1][:min(3, len(self.config['classes']))]
            print(f"   Top {len(top_indices)} secteurs:")
            for idx in top_indices:
                print(f"      ‚Ä¢ {self.config['classes'][idx]}: {probas[0][idx]:.2%}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Impossible de tester avec CV r√©aliste: {e}")
            # On ne fait pas √©chouer le test car c'est juste un exemple


class TestModelIntegrity(unittest.TestCase):
    """Tests d'int√©grit√© des fichiers"""
    
    def test_all_files_exist(self):
        """V√©rifier que tous les fichiers existent"""
        print("\nüîç Test: Int√©grit√© des fichiers")
        
        models_dir = Path(__file__).parent.parent / 'models'
        required_files = [
            'knn_classification_secteur.pkl',
            'scaler_classification_secteur.pkl',
            'knn_classification_secteur_config.pkl'
        ]
        
        for file in required_files:
            file_path = models_dir / file
            self.assertTrue(file_path.exists(), f"Fichier manquant: {file}")
            print(f"   ‚úÖ {file}")


def run_tests():
    """Fonction pour ex√©cuter les tests"""
    # Cr√©er une suite de tests
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Ajouter les tests
    suite.addTests(loader.loadTestsFromTestCase(TestKNNClassification))
    suite.addTests(loader.loadTestsFromTestCase(TestModelIntegrity))
    
    # Ex√©cuter les tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # R√©sum√©
    print("\n" + "="*80)
    print("üìä R√âSUM√â DES TESTS KNN")
    print("="*80)
    print(f"‚úÖ Tests r√©ussis: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"‚ùå Tests √©chou√©s: {len(result.failures)}")
    print(f"‚ö†Ô∏è  Erreurs: {len(result.errors)}")
    print("="*80)
    
    return result.wasSuccessful()


if __name__ == '__main__':
    success = run_tests()
    sys.exit(0 if success else 1)